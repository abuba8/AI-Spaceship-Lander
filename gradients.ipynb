{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from neural_network.layer import DenseLayer\n",
    "from neural_network.activation import Sigmoid\n",
    "from neural_network.loss import MSE, RMSE\n",
    "from neural_network.optimizer import SgdMomentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Gradient:\n",
      " [[1.04993585e-01 1.76627062e-02 2.46650929e-03 3.35237671e-04\n",
      "  4.53958077e-05]]\n",
      "\n",
      "Numerical Gradient:\n",
      " [[1.04993680e-01 1.76625381e-02 2.46647147e-03 3.35065309e-04\n",
      "  4.52970994e-05]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# checking analytical and numerical gradients for sigmoid activation function\n",
    "def func(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def analytical_gradient(func, x):\n",
    "    return func(x) * (1 - func(x))\n",
    "\n",
    "def numerical_gradient(func, x, h=1e-9):\n",
    "    x = np.asarray(x)\n",
    "    return (func(x + h) - func(x)) / h\n",
    "\n",
    "x = np.array([[2,4,6,8,10]])\n",
    "analytical_grad = analytical_gradient(func, x)\n",
    "numerical_grad = numerical_gradient(func, x)\n",
    "\n",
    "# Compare the gradients\n",
    "print(\"Analytical Gradient:\\n\", analytical_grad[:5])\n",
    "print(\"\\nNumerical Gradient:\\n\", numerical_grad[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Gradient:\n",
      " [[ 4  8 12 16 20]]\n",
      "\n",
      "Numerical Gradient:\n",
      " [[ 4.00000033  8.00000066 12.00000099 16.00000132 20.00000165]]\n"
     ]
    }
   ],
   "source": [
    "# checking analytical and numerical gradients for squared function\n",
    "import numpy as np\n",
    "\n",
    "def func(x):\n",
    "    return x**2\n",
    "\n",
    "def analytical_gradient(func, x):\n",
    "    return 2*x\n",
    "\n",
    "def numerical_gradient(func, x, h=1e-9):\n",
    "    x = np.asarray(x)\n",
    "    return (func(x + h) - func(x)) / h\n",
    "\n",
    "x = np.array([[2,4,6,8,10]])\n",
    "analytical_grad = analytical_gradient(func, x)\n",
    "numerical_grad = numerical_gradient(func, x)\n",
    "\n",
    "# Compare the gradients\n",
    "print(\"Analytical Gradient:\\n\", analytical_grad[:5])\n",
    "print(\"\\nNumerical Gradient:\\n\", numerical_grad[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Gradient:\n",
      " [[0 0 0 2 2]]\n",
      "\n",
      "Numerical Gradient:\n",
      " [[[0.         0.         0.         2.00000017 2.00000017]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# checking analytical and numerical gradients for difference in squared function (using two variables)\n",
    "def func(x, y):\n",
    "    return (x-y)**2\n",
    "\n",
    "def analytical_gradient(func, x, y):\n",
    "    return 2*(x-y)\n",
    "\n",
    "def numerical_gradient(func, x, y, h=1e-9):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    print('here1',func(x + h, y + h))\n",
    "    print('here2',func(x,y))\n",
    "    return (func(x + h, y) - func(x-h,y)) / h\n",
    "\n",
    "def numerical_gradient(func, x, y, h=1e-9):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    grad_x = (func(x + h, y) - func(x - h, y)) / (2 * h)\n",
    "    grad_y = (func(x, y + h) - func(x, y - h)) / (2 * h)\n",
    "\n",
    "    return np.array([grad_x]), np.array([grad_y])\n",
    "\n",
    "\n",
    "x = np.array([[2,4,6,8,10]])\n",
    "y = np.array([[2,4,6,7,9]])\n",
    "analytical_grad = analytical_gradient(func, x, y)\n",
    "numerical_grad_x, numerical_grad_y = numerical_gradient(func, x, y)\n",
    "\n",
    "# Compare the gradients\n",
    "print(\"Analytical Gradient:\\n\", analytical_grad[:5])\n",
    "print(\"\\nNumerical Gradient:\\n\", numerical_grad_x[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Gradient:\n",
      " [[ 0.  0.  0. -2. -2.]]\n",
      "\n",
      "Numerical Gradient:\n",
      " [-0.80000007]\n",
      "\n",
      "Mean of Numerical and Analytical Gradients:  -0.8000000661922968 -0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numerical and analytical gradients for MSE\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def analytical_gradient(y_true, y_pred):\n",
    "    return - 2 * (y_true - y_pred) / len(y_true)\n",
    "\n",
    "def numerical_gradient(func, x, y, h=1e-9):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    grad_x = (func(x + h, y) - func(x - h, y)) / (2 * h)\n",
    "    grad_y = (func(x, y + h) - func(x, y - h)) / (2 * h)\n",
    "\n",
    "    return np.array([grad_x]), np.array([grad_y])\n",
    "\n",
    "x_values = np.array([[2,4,6,8,10]])\n",
    "x_pred = np.array([[2,4,6,7,9]])\n",
    "analytical_grad = analytical_gradient(x_values, x_pred)\n",
    "numerical_grad_x, numerical_grad_y = numerical_gradient(mse, x_values, x_pred)\n",
    "\n",
    "# Compare the gradients\n",
    "print(\"Analytical Gradient:\\n\", analytical_grad[:5])\n",
    "print(\"\\nNumerical Gradient:\\n\", numerical_grad_y[:5])\n",
    "print(\"\\nMean of Numerical and Analytical Gradients: \",numerical_grad_y.mean(), analytical_grad.mean(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative for MSE or ERROR in our landing game are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-5.8, -9.7]), array([-5.8, -9.7]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MSE:\n",
    "    def forward(self, y_true, y_pred):\n",
    "        self.loss = np.mean((y_true - y_pred)**2)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        self.dloss = - 2 * (y_true - y_pred) / len(y_true)\n",
    "        return self.dloss\n",
    "    \n",
    "    \n",
    "class Error:\n",
    "    def forward(self, y_true, y_pred):\n",
    "        self.loss = (y_true - y_pred)\n",
    "        return np.mean(self.loss), -self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        self.dloss = - self.loss\n",
    "        return self.dloss\n",
    "    \n",
    "    \n",
    "mse = MSE()\n",
    "error = Error()\n",
    "\n",
    "# for our case the gradients of mse and error are always same, because * and len(y_true) cancels each other\n",
    "y_true = np.array([8.0, 13.0])\n",
    "y_pred = np.array([2.2, 3.3])\n",
    "\n",
    "mse_loss = mse.forward(y_true, y_pred)\n",
    "error, grad = error.forward(y_true, y_pred)\n",
    "mse_dloss = mse.backward(y_true, y_pred)\n",
    "\n",
    "mse_dloss, grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analytical Gradient Mean: -0.7953854235586079\n",
      "Numerical Gradient Mean wrt y: [-0.79538509]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def analytical_gradient_rmse(y_true, y_pred):\n",
    "    denominator = np.sqrt(np.mean((y_true - y_pred)**2, axis=-1))\n",
    "    return - (y_true - y_pred) / (len(y_true) * denominator+ np.finfo(float).eps)\n",
    "\n",
    "def numerical_gradient_rmse(func, x, y, h=1e-9):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    grad_x = (func(x + h, y) - func(x - h, y)) / (2 * h)\n",
    "    grad_y = (func(x, y + h) - func(x, y - h)) / (2 * h)\n",
    "\n",
    "    return np.array([grad_x]), np.array([grad_y])\n",
    "\n",
    "# Test\n",
    "x_values = np.array([[2, 4, 6, 8, 10]])\n",
    "x_pred = np.array([[-1, -2.4, -5.1, -7.6, 9.8]])\n",
    "\n",
    "# RMSE and Analytical Gradient for RMSE\n",
    "rmse_value = rmse(x_values, x_pred)\n",
    "analytical_grad_rmse = analytical_gradient_rmse(x_values, x_pred)\n",
    "numerical_grad_x_rmse, numerical_grad_y_rmse = numerical_gradient_rmse(rmse, x_values, x_pred)\n",
    "\n",
    "# Compare the results\n",
    "print(\"\\nAnalytical Gradient Mean:\", analytical_grad_rmse.mean())\n",
    "print(\"Numerical Gradient Mean wrt y:\", numerical_grad_y_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proving the derivatives for Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14940791 -0.02051583]\n",
      " [ 0.03130677 -0.08540957]] [2. 4.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([2.0, 4.0])\n",
    "obj = DenseLayer(2,2)\n",
    "obj.forward(x)\n",
    "print(obj.w, obj.x) # values of x and w in forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14940791  0.03130677]\n",
      " [-0.02051583 -0.08540957]] [2. 4.]\n"
     ]
    }
   ],
   "source": [
    "print(obj.backward(1), obj.dw) # values for dx and dw in backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward and backward pass for sigmoid function\n",
    "x = np.array([0])\n",
    "act = Sigmoid(lmbda=1.0)\n",
    "act.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act.backward(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
